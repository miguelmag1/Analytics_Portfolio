{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adf376b",
   "metadata": {},
   "source": [
    "Miguel Angel Gomez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313b9f",
   "metadata": {},
   "source": [
    "NLP TWITTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd69f62",
   "metadata": {},
   "source": [
    "**Importando paquetes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c1506a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Miguel Gomez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import fitz #paquete para la lectura de pdf\n",
    "import matplotlib.pyplot as plt\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from nltk import SnowballStemmer\n",
    "import spacy\n",
    "import spacy_spanish_lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "from stop_words import get_stop_words\n",
    "from collections import defaultdict\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "from spacy import displacy\n",
    "import string\n",
    "string.punctuation\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073238ca",
   "metadata": {},
   "source": [
    "Importando Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6831eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tass_Tranin_df= pd.read_csv(\"Tass-train.csv\")\n",
    "Tass_Test_df= pd.read_csv(\"Tass-test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06024eab",
   "metadata": {},
   "source": [
    "**Funciones de limpieza de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8b369df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para borrar emogis\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001F983\"\n",
    "                               u\"\\U0001F9D0\"  # HASTA AQUI\n",
    "                               U\"\\U0001F43E\"\n",
    "                               U\"\\U0001F415\"\n",
    "                               U\"\\U0001F436\"\n",
    "                               U\"\\U0001F43A\"\n",
    "                               U\"\\U0001F98A\"\n",
    "                               U\"\\U0001F429\"\n",
    "                               U\"\\U0001F408\"\n",
    "                               U\"\\U0001F431\"\n",
    "                               U\"\\U0001F638\"\n",
    "                               U\"\\U0001F63B\"\n",
    "                               U\"\\U0001F63C\"\n",
    "                               U\"\\U0001F63F\"\n",
    "                               U\"\\U0001F406\"\n",
    "                               U\"\\U0001F405\"\n",
    "                               U\"\\U0001F42F\"\n",
    "                               U\"\\U0001F98D\"\n",
    "                               U\"\\U0001F412\"\n",
    "                               U\"\\U0001F435\"\n",
    "                               U\"\\U0001F649\"\n",
    "                               U\"\\U0001F648\"\n",
    "                               U\"\\U0001F64A\"\n",
    "                               U\"\\U0001F416\"\n",
    "                               U\"\\U0001F437\"\n",
    "                               U\"\\U0001F43D\"\n",
    "                               U\"\\U0001F40E\"\n",
    "                               U\"\\U0001F3C7\"\n",
    "                               U\"\\U0001F434\"\n",
    "                               U\"\\U0001F410\"\n",
    "                               U\"\\U0001F40F\"\n",
    "                               U\"\\U0001F411\"\n",
    "                               U\"\\U0001F417\"\n",
    "                               U\"\\U0001F98F\"\n",
    "                               U\"\\U0001F418\"\n",
    "                               U\"\\U0001F43C\"\n",
    "                               U\"\\U0001F428\"\n",
    "                               U\"\\U0001F42A\"\n",
    "                               U\"\\U0001F42B\"\n",
    "                               U\"\\U0001F404\"\n",
    "                               U\"\\U0001F42E\"\n",
    "                               U\"\\U0001F402\"\n",
    "                               U\"\\U0001F43B\"\n",
    "                               U\"\\U0001F403\"\n",
    "                               U\"\\U0001F407\"\n",
    "                               U\"\\U0001F430\"\n",
    "                               U\"\\U0001F43F\"\n",
    "                               U\"\\U0001F439\"\n",
    "                               U\"\\U0001F42D\"\n",
    "                               U\"\\U0001F413\"\n",
    "                               U\"\\U0001F414\"\n",
    "                               U\"\\U0001F423\"\n",
    "                               U\"\\U0001F424\"\n",
    "                               U\"\\U0001F983\"\n",
    "                               U\"\\U0001F426\"\n",
    "                               U\"\\U0001F54A\"\n",
    "                               U\"\\U0001F985\"\n",
    "                               U\"\\U0001F989\"\n",
    "                               U\"\\U0001F986\"\n",
    "                               U\"\\U0001F427\"\n",
    "\n",
    "                               U\"\\U0001F422\"\n",
    "                               U\"\\U0001F419\"\n",
    "                               U\"\\U0001F980\"\n",
    "                               U\"\\U0001F990\"\n",
    "                               U\"\\U0001F988\"\n",
    "                               U\"\\U0001F42C\"\n",
    "                               U\"\\U0001F433\"\n",
    "                               U\"\\U0001F40B\"\n",
    "                               U\"\\U0001F41F\"\n",
    "                               U\"\\U0001F420\"\n",
    "                               U\"\\U0001F421\"\n",
    "                               U\"\\U0001F40D\"\n",
    "                               U\"\\U0001F40A\"\n",
    "                               U\"\\U0001F40C\"\n",
    "                               U\"\\U0001F937\"\n",
    "                               U\"\\U0001F912\"\n",
    "                               U\"\\u2B50\"\n",
    "                               U\"\\U0001F914\"\n",
    "                               U\"\\U0001F951\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd761899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para quietar avatar de usuarios\n",
    "\n",
    "def delete_user(string):\n",
    "    at_pattern = re.compile(\"(@[A-Za-z0-9_]+)\")\n",
    "    return at_pattern.sub(r'', string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f7fbd871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para quitar numeros de un txt\n",
    "\n",
    "def quitar_numeros_txt(texto):\n",
    "    texto = re.sub(r'\\d+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85cbaa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "606ff219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion para quitar signos\n",
    "\n",
    "def remove_signos(text):\n",
    "    text_nosignos = \"\".join([char for char in text if char not in '•#¿?!¡.,;:-_\"^()'])\n",
    "    return text_nosignos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d82800eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_español=get_stop_words('es') #obteniendo stop words en español\n",
    "\n",
    "# función para quitar stopwords\n",
    "def quitar_stopwords(texto):\n",
    "    texto = \" \".join([palabra for palabra in texto.split() if palabra not in stop_words_español])\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd7290c",
   "metadata": {},
   "source": [
    "Aplicando las funciones para limpiar datos en la base de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "33f83dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando la funcion para borrar emogis\n",
    "Tass_Tranin_df['content_clean'] = Tass_Tranin_df['content'].apply(remove_emoji)\n",
    "#Usando la funcion para borrar usuarios\n",
    "Tass_Tranin_df['content_clean'] = Tass_Tranin_df['content_clean'].apply(delete_user)\n",
    "#Usando la funcion para borrar numeros\n",
    "Tass_Tranin_df['content_clean'] = Tass_Tranin_df['content_clean'].apply(quitar_numeros_txt)\n",
    "#Usando la funcion para borrar signos de puntuacion\n",
    "Tass_Tranin_df['content_clean'] = Tass_Tranin_df['content_clean'].apply(remove_signos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "86b38d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando la funcion para borrar stopwords de Tass_Tranin_df\n",
    "Tass_Tranin_df['content_clean'] = Tass_Tranin_df['content_clean'].apply(quitar_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72dd78",
   "metadata": {},
   "source": [
    "Aplicando las funciones para limpiar datos en la base de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6836e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usando la funcion para borrar emogis\n",
    "Tass_Test_df['content_clean'] = Tass_Test_df['content'].apply(remove_emoji)\n",
    "#Usando la funcion para borrar usuarios\n",
    "Tass_Test_df['content_clean'] = Tass_Test_df['content_clean'].apply(delete_user)\n",
    "#Usando la funcion para borrar numeros\n",
    "Tass_Test_df['content_clean'] = Tass_Test_df['content_clean'].apply(quitar_numeros_txt)\n",
    "#Usando la funcion para borrar signos de puntuacion\n",
    "#Tass_Test_df['content_clean'] = Tass_Test_df['content_clean'].apply(remove_signos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f4faa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la funcion para borrar stopwords de Tass_Test_df\n",
    "Tass_Test_df['content_clean'] = Tass_Test_df['content_clean'].apply(quitar_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf140e",
   "metadata": {},
   "source": [
    "Tokenizacion de los datos de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d48411f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN DE LAS NOTICIAS DE TASS_TRAIN_DF\n",
    "Tass_Tranin_df['content_clean_Token'] = Tass_Tranin_df['content_clean'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# TOKEN DE LAS NOTICIAS DE Tass_Test_df\n",
    "Tass_Test_df['content_clean_Token'] = Tass_Test_df['content_clean'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7284a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_clean_Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>772432598027145216</td>\n",
       "      <td>71546415</td>\n",
       "      <td>Sin ser fan de Juan Gabriel, siempre supe que ...</td>\n",
       "      <td>Sun Sep 04 13:54:17 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>Sin ser fan Juan Gabriel siempre supe fuerza n...</td>\n",
       "      <td>[Sin, ser, fan, Juan, Gabriel, siempre, supe, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>771715645843079169</td>\n",
       "      <td>106919551</td>\n",
       "      <td>ayer preguntaban y dónde están las solteras!!!...</td>\n",
       "      <td>Fri Sep 02 14:25:22 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>ayer preguntaban dónde solteras grupo alza man...</td>\n",
       "      <td>[ayer, preguntaban, dónde, solteras, grupo, al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>771744506861461504</td>\n",
       "      <td>599653674</td>\n",
       "      <td>Que el finde sea para hacer cualquier cosa que...</td>\n",
       "      <td>Fri Sep 02 16:20:03 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>Que finde hacer cualquier cosa haga FELIZ reve...</td>\n",
       "      <td>[Que, finde, hacer, cualquier, cosa, haga, FEL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>771521266599604224</td>\n",
       "      <td>599653674</td>\n",
       "      <td>Elige amar, así duela, así parezca un imposibl...</td>\n",
       "      <td>Fri Sep 02 01:32:58 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>Elige amar así duela así parezca imposible así...</td>\n",
       "      <td>[Elige, amar, así, duela, así, parezca, imposi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>774703192105971712</td>\n",
       "      <td>280686132</td>\n",
       "      <td>Hoy me sentí como grace de \"al fondo hay sitio...</td>\n",
       "      <td>Sat Sep 10 20:16:48 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "      <td>Hoy sentí grace fondo sitio atropellaron solo ...</td>\n",
       "      <td>[Hoy, sentí, grace, fondo, sitio, atropellaron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>774612388435795968</td>\n",
       "      <td>714677089589452804</td>\n",
       "      <td>hoy me toco visitar a mamá en su descanso eter...</td>\n",
       "      <td>Sat Sep 10 14:15:59 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>hoy toco visitar mamá descanso eterno contare ...</td>\n",
       "      <td>[hoy, toco, visitar, mamá, descanso, eterno, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>774804336236978177</td>\n",
       "      <td>453529360</td>\n",
       "      <td>Las remuneraciones económicas son lo que todos...</td>\n",
       "      <td>Sun Sep 11 02:58:43 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>Las remuneraciones económicas esperan encanta ...</td>\n",
       "      <td>[Las, remuneraciones, económicas, esperan, enc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>773691234871148544</td>\n",
       "      <td>2178283473</td>\n",
       "      <td>@sadalienpotato ¿Opto por el rapado? En serio,...</td>\n",
       "      <td>Thu Sep 08 01:15:39 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>Opto rapado En serio confusa Y admito corto ab...</td>\n",
       "      <td>[Opto, rapado, En, serio, confusa, Y, admito, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>775087224857567232</td>\n",
       "      <td>714677089589452804</td>\n",
       "      <td>Lo que mas amo de mi escritorio es que hay una...</td>\n",
       "      <td>Sun Sep 11 21:42:49 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>P</td>\n",
       "      <td>Lo mas amo escritorio foto juntos aparte cuart...</td>\n",
       "      <td>[Lo, mas, amo, escritorio, foto, juntos, apart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>768663354537828352</td>\n",
       "      <td>423910199</td>\n",
       "      <td>No sé si los maltrataban de verdad pero me dio...</td>\n",
       "      <td>Thu Aug 25 04:16:39 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>N</td>\n",
       "      <td>No sé si maltrataban verdad dio penita ver No ...</td>\n",
       "      <td>[No, sé, si, maltrataban, verdad, dio, penita,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  level_0  index             tweetid                user  \\\n",
       "0           0        0      0  772432598027145216            71546415   \n",
       "1           1        1      1  771715645843079169           106919551   \n",
       "2           2        2      2  771744506861461504           599653674   \n",
       "3           3        3      3  771521266599604224           599653674   \n",
       "4           4        4      4  774703192105971712           280686132   \n",
       "5           5        5      6  774612388435795968  714677089589452804   \n",
       "6           6        6      7  774804336236978177           453529360   \n",
       "7           7        7      8  773691234871148544          2178283473   \n",
       "8           8        8      9  775087224857567232  714677089589452804   \n",
       "9           9        9     14  768663354537828352           423910199   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sin ser fan de Juan Gabriel, siempre supe que ...   \n",
       "1  ayer preguntaban y dónde están las solteras!!!...   \n",
       "2  Que el finde sea para hacer cualquier cosa que...   \n",
       "3  Elige amar, así duela, así parezca un imposibl...   \n",
       "4  Hoy me sentí como grace de \"al fondo hay sitio...   \n",
       "5  hoy me toco visitar a mamá en su descanso eter...   \n",
       "6  Las remuneraciones económicas son lo que todos...   \n",
       "7  @sadalienpotato ¿Opto por el rapado? En serio,...   \n",
       "8  Lo que mas amo de mi escritorio es que hay una...   \n",
       "9  No sé si los maltrataban de verdad pero me dio...   \n",
       "\n",
       "                             date lang sentiment  \\\n",
       "0  Sun Sep 04 13:54:17 +0000 2016   es         P   \n",
       "1  Fri Sep 02 14:25:22 +0000 2016   es       NEU   \n",
       "2  Fri Sep 02 16:20:03 +0000 2016   es       NEU   \n",
       "3  Fri Sep 02 01:32:58 +0000 2016   es       NEU   \n",
       "4  Sat Sep 10 20:16:48 +0000 2016   es         N   \n",
       "5  Sat Sep 10 14:15:59 +0000 2016   es         P   \n",
       "6  Sun Sep 11 02:58:43 +0000 2016   es         P   \n",
       "7  Thu Sep 08 01:15:39 +0000 2016   es       NEU   \n",
       "8  Sun Sep 11 21:42:49 +0000 2016   es         P   \n",
       "9  Thu Aug 25 04:16:39 +0000 2016   es         N   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Sin ser fan Juan Gabriel siempre supe fuerza n...   \n",
       "1  ayer preguntaban dónde solteras grupo alza man...   \n",
       "2  Que finde hacer cualquier cosa haga FELIZ reve...   \n",
       "3  Elige amar así duela así parezca imposible así...   \n",
       "4  Hoy sentí grace fondo sitio atropellaron solo ...   \n",
       "5  hoy toco visitar mamá descanso eterno contare ...   \n",
       "6  Las remuneraciones económicas esperan encanta ...   \n",
       "7  Opto rapado En serio confusa Y admito corto ab...   \n",
       "8  Lo mas amo escritorio foto juntos aparte cuart...   \n",
       "9  No sé si maltrataban verdad dio penita ver No ...   \n",
       "\n",
       "                                 content_clean_Token  \n",
       "0  [Sin, ser, fan, Juan, Gabriel, siempre, supe, ...  \n",
       "1  [ayer, preguntaban, dónde, solteras, grupo, al...  \n",
       "2  [Que, finde, hacer, cualquier, cosa, haga, FEL...  \n",
       "3  [Elige, amar, así, duela, así, parezca, imposi...  \n",
       "4  [Hoy, sentí, grace, fondo, sitio, atropellaron...  \n",
       "5  [hoy, toco, visitar, mamá, descanso, eterno, c...  \n",
       "6  [Las, remuneraciones, económicas, esperan, enc...  \n",
       "7  [Opto, rapado, En, serio, confusa, Y, admito, ...  \n",
       "8  [Lo, mas, amo, escritorio, foto, juntos, apart...  \n",
       "9  [No, sé, si, maltrataban, verdad, dio, penita,...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tass_Tranin_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2498d81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>user</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_clean_Token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>769690830114357248</td>\n",
       "      <td>744639307906953216</td>\n",
       "      <td>@MundonickLA @mgabrieladfc siempre hermosa mar...</td>\n",
       "      <td>Sun Aug 28 00:19:28 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>siempre hermosa maria gabriela</td>\n",
       "      <td>[siempre, hermosa, maria, gabriela]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>771077379531821057</td>\n",
       "      <td>713834336</td>\n",
       "      <td>El sábado me dijeron \"yo te he visto antes, pe...</td>\n",
       "      <td>Wed Aug 31 20:09:07 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El sábado dijeron \"yo visto antes, hablado por...</td>\n",
       "      <td>[El, sábado, dijeron, ``, yo, visto, antes, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>772489016352669701</td>\n",
       "      <td>53311422</td>\n",
       "      <td>Sabes que no tendrás un buen día cuando lo pri...</td>\n",
       "      <td>Sun Sep 04 17:38:28 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sabes buen día primero haces mañana tratar cam...</td>\n",
       "      <td>[Sabes, buen, día, primero, haces, mañana, tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>771317218634149888</td>\n",
       "      <td>599653674</td>\n",
       "      <td>En situaciones en las que no sepas que hacer, ...</td>\n",
       "      <td>Thu Sep 01 12:02:09 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En situaciones sepas hacer, sumérgete sólo así...</td>\n",
       "      <td>[En, situaciones, sepas, hacer, ,, sumérgete, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>771316436107014144</td>\n",
       "      <td>599653674</td>\n",
       "      <td>El Universo es infinito y como tal quiere que ...</td>\n",
       "      <td>Thu Sep 01 11:59:03 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>El Universo infinito tal quiere tambien seas, ...</td>\n",
       "      <td>[El, Universo, infinito, tal, quiere, tambien,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>771720050017460225</td>\n",
       "      <td>1489810838</td>\n",
       "      <td>Cusco again Días felices #AmoCusco #Urubamba #...</td>\n",
       "      <td>Fri Sep 02 14:42:52 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cusco again Días felices #AmoCusco #Urubamba #...</td>\n",
       "      <td>[Cusco, again, Días, felices, #, AmoCusco, #, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>771519053211004929</td>\n",
       "      <td>783350538</td>\n",
       "      <td>En el examen de geometría me estoy esforzando ...</td>\n",
       "      <td>Fri Sep 02 01:24:11 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>En examen geometría esforzando , bueno bien só...</td>\n",
       "      <td>[En, examen, geometría, esforzando, ,, bueno, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>772626202238578688</td>\n",
       "      <td>91282238</td>\n",
       "      <td>Los putos polos esos que se cruzan en el pecho...</td>\n",
       "      <td>Mon Sep 05 02:43:35 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Los putos polos cruzan pecho tan moda puedo po...</td>\n",
       "      <td>[Los, putos, polos, cruzan, pecho, tan, moda, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>772619014820560896</td>\n",
       "      <td>1694172402</td>\n",
       "      <td>@Trovack @iEnterate vamos por buen camino</td>\n",
       "      <td>Mon Sep 05 02:15:02 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vamos buen camino</td>\n",
       "      <td>[vamos, buen, camino]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>772993402791202816</td>\n",
       "      <td>2393444238</td>\n",
       "      <td>#HaceTiempoYoNo Tengo un novio formal</td>\n",
       "      <td>Tue Sep 06 03:02:43 +0000 2016</td>\n",
       "      <td>es</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#HaceTiempoYoNo Tengo novio formal</td>\n",
       "      <td>[#, HaceTiempoYoNo, Tengo, novio, formal]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweetid                user  \\\n",
       "0           0  769690830114357248  744639307906953216   \n",
       "1           1  771077379531821057           713834336   \n",
       "2           2  772489016352669701            53311422   \n",
       "3           3  771317218634149888           599653674   \n",
       "4           4  771316436107014144           599653674   \n",
       "5           5  771720050017460225          1489810838   \n",
       "6           6  771519053211004929           783350538   \n",
       "7           7  772626202238578688            91282238   \n",
       "8           8  772619014820560896          1694172402   \n",
       "9           9  772993402791202816          2393444238   \n",
       "\n",
       "                                             content  \\\n",
       "0  @MundonickLA @mgabrieladfc siempre hermosa mar...   \n",
       "1  El sábado me dijeron \"yo te he visto antes, pe...   \n",
       "2  Sabes que no tendrás un buen día cuando lo pri...   \n",
       "3  En situaciones en las que no sepas que hacer, ...   \n",
       "4  El Universo es infinito y como tal quiere que ...   \n",
       "5  Cusco again Días felices #AmoCusco #Urubamba #...   \n",
       "6  En el examen de geometría me estoy esforzando ...   \n",
       "7  Los putos polos esos que se cruzan en el pecho...   \n",
       "8          @Trovack @iEnterate vamos por buen camino   \n",
       "9              #HaceTiempoYoNo Tengo un novio formal   \n",
       "\n",
       "                             date lang  sentiment  \\\n",
       "0  Sun Aug 28 00:19:28 +0000 2016   es        NaN   \n",
       "1  Wed Aug 31 20:09:07 +0000 2016   es        NaN   \n",
       "2  Sun Sep 04 17:38:28 +0000 2016   es        NaN   \n",
       "3  Thu Sep 01 12:02:09 +0000 2016   es        NaN   \n",
       "4  Thu Sep 01 11:59:03 +0000 2016   es        NaN   \n",
       "5  Fri Sep 02 14:42:52 +0000 2016   es        NaN   \n",
       "6  Fri Sep 02 01:24:11 +0000 2016   es        NaN   \n",
       "7  Mon Sep 05 02:43:35 +0000 2016   es        NaN   \n",
       "8  Mon Sep 05 02:15:02 +0000 2016   es        NaN   \n",
       "9  Tue Sep 06 03:02:43 +0000 2016   es        NaN   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0                     siempre hermosa maria gabriela   \n",
       "1  El sábado dijeron \"yo visto antes, hablado por...   \n",
       "2  Sabes buen día primero haces mañana tratar cam...   \n",
       "3  En situaciones sepas hacer, sumérgete sólo así...   \n",
       "4  El Universo infinito tal quiere tambien seas, ...   \n",
       "5  Cusco again Días felices #AmoCusco #Urubamba #...   \n",
       "6  En examen geometría esforzando , bueno bien só...   \n",
       "7  Los putos polos cruzan pecho tan moda puedo po...   \n",
       "8                                  vamos buen camino   \n",
       "9                 #HaceTiempoYoNo Tengo novio formal   \n",
       "\n",
       "                                 content_clean_Token  \n",
       "0                [siempre, hermosa, maria, gabriela]  \n",
       "1  [El, sábado, dijeron, ``, yo, visto, antes, ,,...  \n",
       "2  [Sabes, buen, día, primero, haces, mañana, tra...  \n",
       "3  [En, situaciones, sepas, hacer, ,, sumérgete, ...  \n",
       "4  [El, Universo, infinito, tal, quiere, tambien,...  \n",
       "5  [Cusco, again, Días, felices, #, AmoCusco, #, ...  \n",
       "6  [En, examen, geometría, esforzando, ,, bueno, ...  \n",
       "7  [Los, putos, polos, cruzan, pecho, tan, moda, ...  \n",
       "8                              [vamos, buen, camino]  \n",
       "9          [#, HaceTiempoYoNo, Tengo, novio, formal]  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tass_Test_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d87265e",
   "metadata": {},
   "source": [
    "Lematización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b866eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lematización\n",
    "\n",
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "# Lematización de las noticias de Tass_Tranin_df\n",
    "Tass_Tranin_df['content_clean_lemat'] = Tass_Tranin_df['content_clean_Token'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\n",
    "\n",
    "# Lematización de las noticias de Tass_Test_df\n",
    "Tass_Test_df['content_clean_lemat'] = Tass_Test_df['content_clean_Token'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7bebcf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Sin ser fan Juan Gabriel siempre supe fuerza n...\n",
       "1    ayer preguntaban dónde solteras grupo alza man...\n",
       "2    Que finde hacer cualquier cosa haga FELIZ reve...\n",
       "3    Elige amar así duela así parezca imposible así...\n",
       "4    Hoy sentí grace fondo sitio atropellaron solo ...\n",
       "5    hoy toco visitar mamá descanso eterno contare ...\n",
       "6    Las remuneraciones económicas esperan encanta ...\n",
       "7    Opto rapado En serio confusa Y admito corto ab...\n",
       "8    Lo ma amo escritorio foto junto aparte cuarto ...\n",
       "9    No sé si maltrataban verdad dio penita ver No ...\n",
       "Name: content_clean_lemat, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tass_Tranin_df['content_clean_lemat'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccbc24",
   "metadata": {},
   "source": [
    "### Vectorizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90cfc101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library\n",
    "modelo_conteo = CountVectorizer()\n",
    "modelo_conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12d6f710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1274x4434 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 10307 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array = modelo_conteo.fit_transform(Tass_Tranin_df['content_clean_lemat'])\n",
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68026adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3770)\t1\n",
      "  (0, 3718)\t1\n",
      "  (0, 1577)\t1\n",
      "  (0, 2161)\t1\n",
      "  (0, 1709)\t1\n",
      "  (0, 3743)\t1\n",
      "  (0, 3893)\t1\n",
      "  (0, 1689)\t1\n",
      "  (0, 2714)\t1\n",
      "  (0, 1943)\t1\n",
      "  (0, 1449)\t1\n",
      "  (0, 3399)\t1\n",
      "  (0, 1172)\t1\n",
      "  (0, 4000)\t1\n",
      "  (0, 1364)\t1\n",
      "  (1, 353)\t1\n",
      "  (1, 3223)\t1\n",
      "  (1, 1261)\t1\n",
      "  (1, 3808)\t1\n",
      "  (1, 1795)\t1\n",
      "  (1, 181)\t1\n",
      "  (1, 2460)\t1\n",
      "  (1, 4429)\t1\n",
      "  (1, 4042)\t1\n",
      "  (1, 4323)\t1\n",
      "  :\t:\n",
      "  (1269, 3220)\t1\n",
      "  (1269, 1783)\t1\n",
      "  (1269, 101)\t1\n",
      "  (1270, 3712)\t1\n",
      "  (1270, 613)\t1\n",
      "  (1270, 4271)\t1\n",
      "  (1270, 2515)\t1\n",
      "  (1270, 4061)\t1\n",
      "  (1270, 1119)\t1\n",
      "  (1270, 3579)\t1\n",
      "  (1270, 1769)\t1\n",
      "  (1270, 335)\t1\n",
      "  (1271, 477)\t1\n",
      "  (1271, 2755)\t1\n",
      "  (1271, 1343)\t1\n",
      "  (1271, 3724)\t1\n",
      "  (1271, 2748)\t1\n",
      "  (1272, 1606)\t1\n",
      "  (1272, 361)\t1\n",
      "  (1272, 3878)\t1\n",
      "  (1272, 3972)\t1\n",
      "  (1272, 176)\t1\n",
      "  (1273, 483)\t1\n",
      "  (1273, 1982)\t1\n",
      "  (1273, 3063)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e5d1b4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1274, 4434)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7380f",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4815e847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.9, max_features=1000, min_df=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000)\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d55f735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1274x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6553 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_stem = tfidf_vectorizer.fit_transform(Tass_Tranin_df['content_clean_lemat'])\n",
    "tfidf_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ac6b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.sparse.csr.csr_matrix to numpy array\n",
    "\n",
    "X_array = X_array.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "743d649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a346e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Tass_Tranin_df.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8204d5e7",
   "metadata": {},
   "source": [
    "## MODELADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aa77e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test= train_test_split(X_array,Y , test_size = 0.2 , train_size= 0.8 , random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7e1f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1179      N\n",
       "1120      N\n",
       "427     NEU\n",
       "351       P\n",
       "364     NEU\n",
       "       ... \n",
       "540       N\n",
       "1147      N\n",
       "70        P\n",
       "464     NEU\n",
       "761       N\n",
       "Name: sentiment, Length: 255, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aaab6d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1019, 4434)\n",
      "(255, 4434)\n",
      "(255,)\n",
      "(1019,)\n"
     ]
    }
   ],
   "source": [
    "X_train.shape\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test.shape\n",
    "print(X_test.shape)\n",
    "\n",
    "Y_test.shape\n",
    "print(Y_test.shape)\n",
    "\n",
    "Y_train.shape\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ac97b",
   "metadata": {},
   "source": [
    "## Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b00cb876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d142401e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5686274509803921"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test , Y_test , sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3cc579c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_test , Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04a673f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5014720314033366"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_train , Y_train , sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "02fc8a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "408aadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N' 'N' 'NEU' 'P' 'N' 'NEU' 'N' 'NEU' 'P' 'N' 'P' 'N' 'N' 'P' 'N' 'P' 'N'\n",
      " 'P' 'N' 'P' 'P' 'P' 'P' 'NEU' 'NEU' 'P' 'N' 'N' 'N' 'N' 'N' 'P' 'P' 'P'\n",
      " 'P' 'NEU' 'NEU' 'NEU' 'P' 'P' 'P' 'P' 'P' 'N' 'N' 'N' 'P' 'NEU' 'N' 'N'\n",
      " 'P' 'N' 'P' 'NEU' 'NEU' 'P' 'P' 'P' 'N' 'P' 'P' 'N' 'P' 'NEU' 'P' 'P' 'P'\n",
      " 'P' 'N' 'N' 'N' 'P' 'P' 'P' 'P' 'N' 'P' 'NEU' 'NEU' 'P' 'N' 'N' 'P' 'P'\n",
      " 'NEU' 'N' 'N' 'P' 'NEU' 'N' 'P' 'P' 'N' 'N' 'NEU' 'N' 'P' 'N' 'P' 'NEU'\n",
      " 'N' 'P' 'P' 'P' 'N' 'P' 'P' 'N' 'N' 'P' 'N' 'N' 'N' 'N' 'P' 'N' 'N' 'P'\n",
      " 'NEU' 'P' 'N' 'N' 'P' 'N' 'NEU' 'N' 'N' 'N' 'N' 'P' 'NEU' 'P' 'N' 'NEU'\n",
      " 'NEU' 'NEU' 'N' 'P' 'NEU' 'N' 'N' 'N' 'NEU' 'N' 'N' 'N' 'P' 'N' 'N' 'N'\n",
      " 'P' 'N' 'N' 'NEU' 'N' 'P' 'P' 'P' 'NEU' 'P' 'N' 'P' 'N' 'P' 'N' 'P' 'P'\n",
      " 'P' 'P' 'P' 'P' 'NEU' 'NEU' 'N' 'NEU' 'P' 'NEU' 'NEU' 'N' 'NEU' 'P' 'NEU'\n",
      " 'P' 'P' 'NEU' 'N' 'P' 'NEU' 'NEU' 'N' 'NEU' 'NEU' 'N' 'N' 'P' 'N' 'P' 'P'\n",
      " 'NEU' 'N' 'N' 'P' 'N' 'P' 'N' 'N' 'P' 'P' 'P' 'P' 'NEU' 'P' 'N' 'N' 'P'\n",
      " 'N' 'N' 'N' 'N' 'P' 'P' 'P' 'N' 'P' 'N' 'P' 'N' 'N' 'NEU' 'N' 'P' 'N' 'N'\n",
      " 'N' 'N' 'N' 'N' 'P' 'N' 'P' 'NEU' 'N' 'N' 'P' 'P' 'P' 'N' 'N' 'N' 'N' 'N'\n",
      " 'N' 'P' 'NEU' 'N']\n"
     ]
    }
   ],
   "source": [
    "Y_predict = classifier.predict(X_test)\n",
    "print(Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6521225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[105   0   0]\n",
      " [  4  45   1]\n",
      " [  0   0 100]]\n"
     ]
    }
   ],
   "source": [
    "matrix = confusion_matrix(Y_test , Y_predict)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9ee29090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N      553\n",
       "P      461\n",
       "NEU    260\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tass_Tranin_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47023f",
   "metadata": {},
   "source": [
    "## Complement NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ac4368f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import ComplementNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39ab5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ComplementNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bede00da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dd22b519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9744847890088322"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train , Y_train , sample_weight = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bbf3aa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5725490196078431"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test , Y_test , sample_weight = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7365a2b3",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3cbc6408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50, random_state=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth=50, random_state=0)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "12d1d271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8155053974484789"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train , Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2159727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5411764705882353"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test , Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971bb79",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5a4bc214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 0.5568627450980392\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "classifier.fit(X_train, Y_train)\n",
    "score = classifier.score(X_test, Y_test)\n",
    "\n",
    "print(\"Precisión:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a09ddb",
   "metadata": {},
   "source": [
    "### red neuronal relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7fac76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d666afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "03e314cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5220370906983482"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model, X_train, Y_train, cv = 5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b669fd6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4980392156862745"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(model, X_test, Y_test, cv = 5).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "43dbe3687308bc9f9b1170eeda74f2f07b7727c21844e39cf70f913d86551339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
